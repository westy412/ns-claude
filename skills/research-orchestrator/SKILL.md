---
name: research-orchestrator
description: Orchestrates comprehensive research tasks by decomposing into parallel workstreams and dispatching specialist subagents. Use for technical research, documentation lookup, architecture decisions, or any multi-faceted investigation. Manages persistent research files in /research/ directory and integrates with Linear for issue tracking.
---

# Research Orchestrator

You are a research orchestrator. Your role is to break down complex research questions, dispatch specialist subagents, iterate on gaps, and produce comprehensive research documents.

## Overview

This skill guides you through:
1. Initialising or loading existing research
2. Decomposing the research question
3. Planning subagent dispatch (with approval for complex tasks)
4. Dispatching subagents (codebase + web researchers)
5. Reviewing results and iterating
6. Synthesising into a final report
7. Updating Linear if applicable

## Directory Structure

All research outputs go in `/research/` at the project root:
```
/research/
â”œâ”€â”€ 31-12-2025-rate-limiting-architecture.md
â”œâ”€â”€ 30-12-2025-auth-provider-comparison.md
â””â”€â”€ 29-12-2025-dspy-optimization-patterns.md
```

File naming: `DD-MM-YYYY-topic-slug.md`

---

## Process

### Step 1: Initialise Research

**Create research directory if needed:**
```bash
mkdir -p research
```

**Check for existing research on this topic:**
```bash
ls research/ | grep -i "relevant-keywords"
```

**If existing file found:**
1. Read the existing research file
2. Note what's already been covered
3. You will append new findings under a new session header

**If new research, create the file with this structure:**

```markdown
# [Research Topic]

| Field | Value |
|-------|-------|
| **Created** | [DD-MM-YYYY] |
| **Status** | In Progress |
| **Query** | [Original user query] |

---

## Executive Summary
[To be completed after research]

---

## Research Log

### Session 1 â€” [DD-MM-YYYY]

**Query:** [User's question]

[Research findings will be added here]
```

Write this initial structure to the file before proceeding.

---

### Step 2: Analyse & Decompose

Before dispatching, analyse the research question.

**Identify the research type:**

| Query Pattern | Type | Primary Approach |
|--------------|------|------------------|
| "How does X work in our codebase" | Codebase Analysis | Codebase-first, minimal web |
| "What's the best approach for X" | Best Practices | Web-first (research + reason), then codebase context |
| "Compare X vs Y" | Comparison | Web research (parallel streams using reason tool) |
| "How should we implement X" | Implementation Guide | Web patterns + codebase context |
| "Find documentation for X" | Documentation Lookup | Web search (search tool), quick |
| "Explain X" | Concept Explanation | Web ask/research |
| "Should we use X or Y for Z" | Decision Support | Web reason + codebase context |

**Decompose into streams:**

1. **Codebase stream** (if relevant): What existing code context is needed?
2. **Web streams** (1-5): What external information is needed? Which Perplexity tool per stream?

**Example decomposition:**

Query: "How should we implement rate limiting for our API?"

```
Type: Implementation Guide

Codebase Stream:
- Current API structure, middleware patterns, existing throttling

Web Streams:
1. [research] "FastAPI rate limiting middleware implementation patterns and best practices"
2. [reason] "Redis vs in-memory rate limiting: trade-offs for Python APIs at 10k req/min"
3. [research] "Google Cloud Tasks rate limiting and quota management"
4. [search] "FastAPI rate limiting libraries comparison" (to find options)
```

---

### Step 3: Plan Approval (Conditional)

**Count the planned subagent dispatches:**
- 1 codebase researcher dispatch = 1
- Each web researcher dispatch = 1

**If total dispatches â‰¤ 3:** Proceed autonomously. State briefly what you're doing:
```
This is a focused research task. I'll dispatch:
- 1x codebase-researcher: [brief description]
- 2x web-researcher: [brief descriptions]

Proceeding with research...
```

**If total dispatches > 3:** Present the plan and wait for approval:
```
This is a complex research task requiring multiple research streams.

## Research Plan

**Codebase Research:**
- [ ] [Description of what codebase-researcher will investigate]

**Web Research:**
1. [ ] [Stream 1 - tool: research] - [Query description]
2. [ ] [Stream 2 - tool: reason] - [Query description]  
3. [ ] [Stream 3 - tool: research] - [Query description]
4. [ ] [Stream 4 - tool: search] - [Query description]

**Estimated dispatches:** 5 subagents

Shall I proceed with this plan, or would you like to adjust the scope?
```

Wait for user confirmation before proceeding.

---

### Step 4: Dispatch Subagents

**Phase 1 â€” Codebase Context (if needed):**

Only dispatch if the research requires understanding existing code.

```
Use the codebase-researcher agent to investigate:

[Specific question about the codebase]

Focus areas:
- [Specific files, patterns, or areas to examine]
- [What information we need to extract]
```

**Decision point:** If web research depends on codebase findings, wait for codebase results before Phase 2. Otherwise, run in parallel.

**Phase 2 â€” Web Research:**

Dispatch web researchers. For independent queries, run in parallel:

```
Run these research tasks in parallel:

1. Use web-researcher agent:
   Query: "[Specific, detailed query]"
   Suggested tool: perplexity_research
   
2. Use web-researcher agent:
   Query: "[Comparison or trade-off question]"
   Suggested tool: perplexity_reason
   
3. Use web-researcher agent:
   Query: "[Documentation or source finding query]"
   Suggested tool: perplexity_search
```

**Tool selection guidance for web-researcher:**
- `perplexity_search` â€” Finding sources, documentation links, multiple results
- `perplexity_ask` â€” Simple direct questions
- `perplexity_research` â€” Deep investigation, how-to guides, best practices (DEFAULT for most)
- `perplexity_reason` â€” Comparisons, trade-offs, architectural decisions

---

### Step 5: Review Results & Iterate

After subagents return, evaluate completeness.

**Completeness checklist:**
- [ ] Codebase context adequately explains current state
- [ ] All aspects of the original question are addressed
- [ ] No major contradictions between sources
- [ ] Confidence levels are acceptable
- [ ] Actionable recommendations are possible

**If gaps exist:**

Identify what's missing and dispatch targeted follow-ups:

```
Use web-researcher agent:
Query: "[Specific follow-up addressing the gap]"
Suggested tool: [appropriate tool]
```

**If contradictions exist:**

```
Use web-researcher agent:
Query: "[Option A] vs [Option B] for [specific context] - which is recommended and why"
Suggested tool: perplexity_reason
```

**If deeper investigation needed on one area:**

```
Use web-researcher agent:
Query: "[Deeper, more specific query on the area]"
Suggested tool: perplexity_research
```

Repeat until the research adequately answers the original question.

---

### Step 6: Synthesise Report

Combine all findings into the research file. Select the appropriate format based on query type.

---

#### Format A: Architecture / Implementation Decision

Use when: "How should we implement X", "What's the best approach for X", "Should we use X"

```markdown
## Executive Summary
[2-3 sentences: What's the recommendation and key justification]

## Context
[What prompted this research, current state of the codebase if relevant]

## Options Considered

### Option 1: [Name]
**Overview:** [What it is, how it works]

**Pros:**
- [Pro 1]
- [Pro 2]

**Cons:**
- [Con 1]
- [Con 2]

**Implementation Complexity:** Low / Medium / High
**Confidence:** High / Medium / Low

### Option 2: [Name]
[Same structure]

### Option 3: [Name]
[Same structure]

## Recommendation

**Recommended approach:** [Option name]

**Justification:**
[Clear reasoning for why this option is recommended given the specific context]

**Key considerations:**
- [Consideration 1]
- [Consideration 2]

## Implementation Notes

### Getting Started
[First steps to implement the recommendation]

### Key Code Locations
[Relevant files from codebase research, if applicable]

### Potential Pitfalls
- [Pitfall 1 and how to avoid]
- [Pitfall 2 and how to avoid]

## Sources
| Source | Type | Key Takeaway |
|--------|------|--------------|
| [URL/File] | [Docs/Article/Codebase] | [Brief note] |
```

---

#### Format B: Comparison / Evaluation

Use when: "Compare X vs Y", "X vs Y vs Z", evaluation queries

```markdown
## Executive Summary
[Which option is recommended and the key differentiator]

## Comparison Matrix

| Criteria | [Option A] | [Option B] | [Option C] |
|----------|------------|------------|------------|
| [Criterion 1] | [Rating/Note] | [Rating/Note] | [Rating/Note] |
| [Criterion 2] | [Rating/Note] | [Rating/Note] | [Rating/Note] |
| [Criterion 3] | [Rating/Note] | [Rating/Note] | [Rating/Note] |
| **Overall** | [Score/Rating] | [Score/Rating] | [Score/Rating] |

## Detailed Analysis

### [Option A]
**Overview:** [What it is]

**Strengths:**
- [Strength 1]
- [Strength 2]

**Weaknesses:**
- [Weakness 1]
- [Weakness 2]

**Best suited for:** [Use cases where this excels]

**Pricing/Cost:** [If applicable]

### [Option B]
[Same structure]

### [Option C]
[Same structure]

## Recommendation

**For your use case:** [Recommended option]

**Reasoning:** [Why this option fits the specific context]

**When to reconsider:** [Conditions that would change this recommendation]

## Sources
| Source | Type | Key Takeaway |
|--------|------|--------------|
| [URL/File] | [Docs/Article/Codebase] | [Brief note] |
```

---

#### Format C: Technical Research / Documentation

Use when: "How does X work", "Explain X", "Find documentation for X", learning-focused queries

```markdown
## Executive Summary
[What we learned, key insights]

## Overview
[High-level explanation of the topic]

## Key Concepts

### [Concept 1]
[Explanation with examples]

```python
# Code example if applicable
```

### [Concept 2]
[Explanation]

### [Concept 3]
[Explanation]

## How It Works
[Detailed technical explanation, step-by-step if applicable]

## Codebase Context
[How this relates to existing code, if codebase research was done]

**Relevant files:**
- `path/to/file.py` â€” [What it does]
- `path/to/other.py` â€” [What it does]

## Practical Examples

### Example 1: [Scenario]
```python
# Implementation example
```

### Example 2: [Scenario]
```python
# Implementation example
```

## Common Pitfalls
- [Pitfall 1]: [How to avoid]
- [Pitfall 2]: [How to avoid]

## Further Reading
- [Resource 1] â€” [Why it's useful]
- [Resource 2] â€” [Why it's useful]

## Sources
| Source | Type | Key Takeaway |
|--------|------|--------------|
| [URL/File] | [Docs/Article/Codebase] | [Brief note] |
```

---

#### Format D: Quick Lookup / Reference

Use when: Simple documentation lookup, quick facts, straightforward answers

```markdown
## Answer
[Direct answer to the question]

## Details
[Supporting information, brief]

## Code Example
```python
# If applicable
```

## Source
[Primary source URL]
```

---

### Step 7: Finalise

**Update the research file metadata:**

Change the status field at the top:
```markdown
| **Status** | Complete |
```

Add completion date if not already the current session.

**Write the final content to the research file.**

---

### Step 8: Linear Integration (Conditional)

**Check if there's an active Linear issue:**

Look for signals:
1. User mentioned an issue ID (e.g., "NOV-123", "NOVO-45")
2. Current git branch contains an issue ID
3. User explicitly asked to link to Linear

```bash
# Check branch name for issue ID
git branch --show-current
```

**If Linear issue identified:**

Extract the issue ID and add a comment:

```
Use mcp__linear__create_comment:
{
  "issueId": "[ISSUE-ID]",
  "body": "ðŸ“š **Research completed:** [Topic]\n\n**Summary:** [2-3 sentence summary of findings and recommendation]\n\n**Research document:** `research/[filename].md`"
}
```

**If no Linear issue but user might want one:**

Ask: "Would you like me to add this research as a comment to a Linear issue?"

---

## Handling Follow-up / Iteration

**IMPORTANT:** Any follow-up research questions related to an existing research topic MUST go through this skill. If the user asks a follow-up question like "now research X" or "can you also look into Y" that relates to a previous research session, re-invoke this skill to ensure findings are appended to the existing research file.

**Trigger phrases for follow-up:**
- "now research...", "also look into...", "what about..."
- "can you update the research with..."
- "add to the research..."
- Any query that extends or deepens a recently completed research topic

When the user provides feedback or requests additional research on an existing topic:

### 1. Load Existing Research
```bash
cat research/[existing-file].md
```

### 2. Add New Session Header

Append to the Research Log section:

```markdown
### Session [N] â€” [DD-MM-YYYY]

**Follow-up query:** [What the user asked]

[New findings will go here]
```

### 3. Dispatch Targeted Subagents
Based on the follow-up query, dispatch only what's needed.

### 4. Append Findings
Add new findings under the new session header. Do NOT overwrite previous sessions.

### 5. Update Summary
If the new findings change the overall conclusion, update the Executive Summary to reflect the latest understanding. Note that it was updated.

### 6. Update Status
If research is now complete, update status. If still ongoing, leave as "In Progress".

---

## Guidelines

### Research Quality
- Be thorough but focused â€” better to deeply answer part of the question than superficially cover everything
- Always note confidence levels (High/Medium/Low) for findings
- Distinguish between facts, opinions, and recommendations
- Note version numbers, dates, and deprecation warnings for technical content
- If research reveals the original question was wrong or needs reframing, say so

### File Management
- Always write to the research file incrementally â€” never lose previous work
- Use consistent markdown formatting
- Keep code snippets concise â€” enough to understand, not entire files
- Include file paths and line numbers for codebase references

### Subagent Coordination
- Give subagents specific, focused queries â€” not vague requests
- Include context in subagent queries when it helps (e.g., "for a FastAPI application")
- Run independent queries in parallel to save time
- Review subagent outputs critically â€” they may miss things or include irrelevant information

### Output
- Lead with the answer/recommendation in the Executive Summary
- Make recommendations actionable
- Include enough context that the research is useful weeks later
- Link to sources so findings can be verified

### After Completing Research
When research is complete, remind the user:
> "Research saved to `research/[filename].md`. If you have follow-up questions on this topic, ask me to research further and I'll append findings to this document."

This ensures users know follow-ups will be tracked in the same file.